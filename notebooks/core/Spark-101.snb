{
  "metadata" : {
    "name" : "Spark 101",
    "user_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "auto_save_timestamp" : "2015-01-10T00:02:12.659Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : null,
    "customImports" : null,
    "customSparkConf" : null
  },
  "cells" : [ {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "<style>\n  h1, h2, h3, h4, h5, p, ul, li {\n    color: #2C475C;\n  }\n  .output_html {\n    color: skyblue;\n  }\n  hr { height: 2px; color: lightblue; }\n</style>"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "# Spark 101"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark._\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.rdd._",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark._\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.rdd._\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 5
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### First create a dataset using the local `syslog` file"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "We will \n\n*  load the file\n*  convert each line keeping its size\n*  remove the duplicates"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "For that, we'll use the `sparkContext`, which\n\n* is the driver\n* can define job (read inputs, transform, group, etc)\n* constructs DAG\n* schedules tasks on the cluster"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val dta:RDD[Int] = sparkContext.textFile(\"/var/log/syslog\")\n                               .map(_.size)\n                               .distinct",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "dta: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[17] at distinct at <console>:54\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "MapPartitionsRDD[17] at distinct at &lt;console&gt;:54"
      },
      "output_type" : "execute_result",
      "execution_count" : 6
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "**MappedRDD** is actually an instance of `RDD[Int]` that will contain the distinct sizes of the lines."
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "_Note_: there is NO computations happening! → [see UI](http://localhost:4040/stages/)"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "-----"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Now we can use the size for fancy operations like grouping per last digit"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val rdd1:RDD[(Int, Iterable[Int])] = dta.groupBy(_ % 10)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "rdd1: org.apache.spark.rdd.RDD[(Int, Iterable[Int])] = ShuffledRDD[19] at groupBy at <console>:54\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "ShuffledRDD[19] at groupBy at &lt;console&gt;:54"
      },
      "output_type" : "execute_result",
      "execution_count" : 7
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### But we can also get rid of even sizes (... non trivially...), then _tupling_ with some other computations"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val rdd2 = dta.map(_ + 1)\n              .filter(_ % 2 == 0)\n              .map(x => (x%10, x*x))",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "rdd2: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[22] at map at <console>:56\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "MapPartitionsRDD[22] at map at &lt;console&gt;:56"
      },
      "output_type" : "execute_result",
      "execution_count" : 8
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "-----"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### We can combine distributed datasets into single ones, by _joining_ them for instance."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val joined = rdd1.join(rdd2)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "joined: org.apache.spark.rdd.RDD[(Int, (Iterable[Int], Int))] = MapPartitionsRDD[25] at join at <console>:58\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "MapPartitionsRDD[25] at join at &lt;console&gt;:58"
      },
      "output_type" : "execute_result",
      "execution_count" : 9
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "_Note (again)_: still nothing done on the cluster up to here → [see ui](http://localhost:4040/stages/)"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "-----"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "#### Now we ask the cluster to do the whole thing: Action"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "joined.take(10).toList.mkString(\"\\n\")",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res2: String = \n(4,(CompactBuffer(84, 134, 124, 114, 94),5476))\n(4,(CompactBuffer(84, 134, 124, 114, 94),8836))\n(4,(CompactBuffer(84, 134, 124, 114, 94),7056))\n(4,(CompactBuffer(84, 134, 124, 114, 94),26896))\n(4,(CompactBuffer(84, 134, 124, 114, 94),33856))\n(4,(CompactBuffer(84, 134, 124, 114, 94),15376))\n(4,(CompactBuffer(84, 134, 124, 114, 94),10816))\n(0,(CompactBuffer(160, 100, 120, 130, 80, 170, 140, 40, 90, 70, 60),14400))\n(0,(CompactBuffer(160, 100, 120, 130, 80, 170, 140, 40, 90, 70, 60),19600))\n(0,(CompactBuffer(160, 100, 120, 130, 80, 170, 140, 40, 90, 70, 60),6400))\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "(4,(CompactBuffer(84, 134, 124, 114, 94),5476))\n(4,(CompactBuffer(84, 134, 124, 114, 94),8836))\n(4,(CompactBuffer(84, 134, 124, 114, 94),7056))\n(4,(CompactBuffer(84, 134, 124, 114, 94),26896))\n(4,(CompactBuffer(84, 134, 124, 114, 94),33856))\n(4,(CompactBuffer(84, 134, 124, 114, 94),15376))\n(4,(CompactBuffer(84, 134, 124, 114, 94),10816))\n(0,(CompactBuffer(160, 100, 120, 130, 80, 170, 140, 40, 90, 70, 60),14400))\n(0,(CompactBuffer(160, 100, 120, 130, 80, 170, 140, 40, 90, 70, 60),19600))\n(0,(CompactBuffer(160, 100, 120, 130, 80, 170, 140, 40, 90, 70, 60),6400))"
      },
      "output_type" : "execute_result",
      "execution_count" : 10
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "_Note (yeah)_: NOW there were some computations in the cluster → [see stages](http://localhost:4040/stages/) and [see tasks](http://localhost:4040/stages/stage/?id=3&attempt=0)"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "-----"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "## But what just happened?"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### First Spark created a DAG based on the job definition"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "joined.toDebugString",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res3: String = \n(2) MapPartitionsRDD[25] at join at <console>:58 []\n |  MapPartitionsRDD[24] at join at <console>:58 []\n |  CoGroupedRDD[23] at join at <console>:58 []\n |  ShuffledRDD[19] at groupBy at <console>:54 []\n +-(2) MapPartitionsRDD[18] at groupBy at <console>:54 []\n    |  MapPartitionsRDD[17] at distinct at <console>:54 []\n    |  ShuffledRDD[16] at distinct at <console>:54 []\n    +-(2) MapPartitionsRDD[15] at distinct at <console>:54 []\n       |  MapPartitionsRDD[14] at map at <console>:53 []\n       |  /var/log/syslog MapPartitionsRDD[13] at textFile at <console>:52 []\n       |  /var/log/syslog HadoopRDD[12] at textFile at <console>:52 []\n +-(2) MapPartitionsRDD[22] at map at <console>:56 []\n    |  MapPartitionsRDD[21] at filter at <console>:55 []\n    |  MapPartitionsRDD[20] a..."
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "(2) MapPartitionsRDD[25] at join at &lt;console&gt;:58 []\n |  MapPartitionsRDD[24] at join at &lt;console&gt;:58 []\n |  CoGroupedRDD[23] at join at &lt;console&gt;:58 []\n |  ShuffledRDD[19] at groupBy at &lt;console&gt;:54 []\n +-(2) MapPartitionsRDD[18] at groupBy at &lt;console&gt;:54 []\n    |  MapPartitionsRDD[17] at distinct at &lt;console&gt;:54 []\n    |  ShuffledRDD[16] at distinct at &lt;console&gt;:54 []\n    +-(2) MapPartitionsRDD[15] at distinct at &lt;console&gt;:54 []\n       |  MapPartitionsRDD[14] at map at &lt;console&gt;:53 []\n       |  /var/log/syslog MapPartitionsRDD[13] at textFile at &lt;console&gt;:52 []\n       |  /var/log/syslog HadoopRDD[12] at textFile at &lt;console&gt;:52 []\n +-(2) MapPartitionsRDD[22] at map at &lt;console&gt;:56 []\n    |  MapPartitionsRDD[21] at filter at &lt;console&gt;:55 []\n    |  MapPartitionsRDD[20] at map at &lt;console&gt;:54 []\n    |  MapPartitionsRDD[17] at distinct at &lt;console&gt;:54 []\n    |  ShuffledRDD[16] at distinct at &lt;console&gt;:54 []\n    +-(2) MapPartitionsRDD[15] at distinct at &lt;console&gt;:54 []\n       |  MapPartitionsRDD[14] at map at &lt;console&gt;:53 []\n       |  /var/log/syslog MapPartitionsRDD[13] at textFile at &lt;console&gt;:52 []\n       |  /var/log/syslog HadoopRDD[12] at textFile at &lt;console&gt;:52 []"
      },
      "output_type" : "execute_result",
      "execution_count" : 11
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Then it scheduled it to the executors in the cluster <small>only one when running in local mode<small>"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "We can check the <strong>Total tasks</strong> activity in the [UI](http://localhost:4040/executors/)"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "-------"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "## Now we will prepare the dataset and then using it several times"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "So we'll read a file about stock price per day, so let's create a type holding relevant data."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "case class Quote(stock:String, date:String, price:Double) extends java.io.Serializable",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "defined class Quote\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 12
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "The file will contain lines like:\n``` \nASTE,2011-12-06,33.93\nASTE,2012-03-14,36.84\n```"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Let's download the data first"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import sys.process._\n\"mkdir -p /tmp/data\"!!\n\nif (!new java.io.File(\"/tmp/data/closes.csv\").exists)\n  \"wget https://s3-eu-west-1.amazonaws.com/spark-notebook-data/closes.csv -O /tmp/data/closes.csv\"!!",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "warning: there were 2 feature warning(s); re-run with -feature for details\nimport sys.process._\nres4: Any = \"\"\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 13
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : ":sh du -h /tmp/data/closes.csv",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "warning: there were 1 feature warning(s); re-run with -feature for details\nimport sys.process._\nres5: scala.xml.Elem = \n<pre>174M\t/tmp/data/closes.csv\n</pre>\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "<pre>174M\t/tmp/data/closes.csv\n</pre>"
      },
      "output_type" : "execute_result",
      "execution_count" : 14
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val closes:RDD[Quote] = sparkContext.textFile(\"/tmp/data/closes.csv\")\n                                   .map(_.split(\",\").toList)\n                                   .map{ case s::d::p::Nil => Quote(s, d, p.toDouble)}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "<console>:62: warning: match may not be exhaustive.\nIt would fail on the following input: List(_, _, _, _)\n                                          .map{ case s::d::p::Nil => Quote(s, d, p.toDouble)}\n                                              ^\ncloses: org.apache.spark.rdd.RDD[Quote] = MapPartitionsRDD[29] at map at <console>:62\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "MapPartitionsRDD[29] at map at &lt;console&gt;:62"
      },
      "output_type" : "execute_result",
      "execution_count" : 15
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "We have date, so we can group stock prices per day"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val byDate = closes.keyBy(_.date)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "byDate: org.apache.spark.rdd.RDD[(String, Quote)] = MapPartitionsRDD[30] at keyBy at <console>:62\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "MapPartitionsRDD[30] at keyBy at &lt;console&gt;:62"
      },
      "output_type" : "execute_result",
      "execution_count" : 16
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Now we can compute the minimum stocks per date"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "def minByDate = byDate.combineByKey[(String, Double)](                                                                                           // `def` to force spark recomputing... otherwise it's smart enough to reuse previous RDDs...\n  (x:Quote) => (x.stock, x.price), \n  (d:(String, Double), l:Quote) => if (d._2 < l.price) d else (l.stock, l.price),\n  (d1:(String, Double), d2:(String, Double)) => if (d1._2 < d2._2) d1 else d2\n)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "minByDate: org.apache.spark.rdd.RDD[(String, (String, Double))]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "ShuffledRDD[31] at combineByKey at &lt;console&gt;:64"
      },
      "output_type" : "execute_result",
      "execution_count" : 17
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "<pre>{minByDate.take(2).toList.mkString(\"\\n\")}</pre>",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res6: scala.xml.Elem = \n<pre>(2012-10-17,(FRCN,0.0))\n(2012-04-05,(FULO,0.0))</pre>\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "<pre>(2012-10-17,(FRCN,0.0))\n(2012-04-05,(FULO,0.0))</pre>"
      },
      "output_type" : "execute_result",
      "execution_count" : 18
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "It took ~2 seconds (in local[8] and 24G of RAM)"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "<pre>{minByDate.take(2).toList.mkString(\"\\n\")}</pre>",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res7: scala.xml.Elem = \n<pre>(2012-10-17,(FRCN,0.0))\n(2012-04-05,(FULO,0.0))</pre>\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "<pre>(2012-10-17,(FRCN,0.0))\n(2012-04-05,(FULO,0.0))</pre>"
      },
      "output_type" : "execute_result",
      "execution_count" : 19
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Once again.... 2 seconds!!!"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "#### Solution: caching!"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val maxByDate2 = byDate.combineByKey[(String, Double)](\n  (x:Quote) => (x.stock, x.price), \n  (d:(String, Double), l:Quote) => if (d._2 > l.price) d else (l.stock, l.price),\n  (d1:(String, Double), d2:(String, Double)) => if (d1._2 > d2._2) d1 else d2\n)\n\nmaxByDate2.cache()                                                                                                               // okay.... not really needed since Spark is smart enough in this case -_-\"",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "maxByDate2: org.apache.spark.rdd.RDD[(String, (String, Double))] = ShuffledRDD[34] at combineByKey at <console>:64\nres8: maxByDate2.type = ShuffledRDD[34] at combineByKey at <console>:64\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "ShuffledRDD[34] at combineByKey at &lt;console&gt;:64"
      },
      "output_type" : "execute_result",
      "execution_count" : 20
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Ask some data"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "<pre>{maxByDate2.take(2).toList.mkString(\"\\n\")}</pre>",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res9: scala.xml.Elem = \n<pre>(2012-10-17,(INLC,4000000.0))\n(2012-04-05,(INOLD,1.0E8))</pre>\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "<pre>(2012-10-17,(INLC,4000000.0))\n(2012-04-05,(INOLD,1.0E8))</pre>"
      },
      "output_type" : "execute_result",
      "execution_count" : 21
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "**Go to [UI](http://localhost:4040/storage/)**"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "<pre>{maxByDate2.take(2).toList.mkString(\"\\n\")}</pre>",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res10: scala.xml.Elem = \n<pre>(2012-10-17,(INLC,4000000.0))\n(2012-04-05,(INOLD,1.0E8))</pre>\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "<pre>(2012-10-17,(INLC,4000000.0))\n(2012-04-05,(INOLD,1.0E8))</pre>"
      },
      "output_type" : "execute_result",
      "execution_count" : 22
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "**BLAZING FAST** => Reuses the cache!"
  } ],
  "nbformat" : 4
}