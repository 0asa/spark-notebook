{
  "metadata" : {
    "name" : "Read 1000Genomes dataset (chr-N)",
    "user_save_timestamp" : "2015-01-02T16:55:37.810Z",
    "auto_save_timestamp" : "2015-01-02T17:15:29.936Z"
  },
  "worksheets" : [ {
    "cells" : [ {
      "cell_type" : "code",
      "input" : ":local-repo /tmp/spark-notebook/repo",
      "language" : "scala",
      "collapsed" : false,
      "prompt_number" : 1,
      "outputs" : [ ]
    }, {
      "cell_type" : "code",
      "input" : "sparkContext.stop",
      "language" : "scala",
      "collapsed" : false,
      "prompt_number" : 7,
      "outputs" : [ ]
    }, {
      "cell_type" : "code",
      "input" : "import sys.process._\nval master = (\"ec2-metadata --public-hostname\"!!).drop(\"public-hostname: \".size).mkString.trim",
      "language" : "scala",
      "collapsed" : false,
      "prompt_number" : 3,
      "outputs" : [ ]
    }, {
      "cell_type" : "code",
      "input" : "//val fs_s3_awsAccessKeyId      = sys.env.get(\"AWS_ACCESS_KEY_ID\").getOrElse(\"<hard-code-one>\")\n//val fs_s3_awsSecretAccessKey  = sys.env.get(\"AWS_SECRET_ACCESS_KEY\").getOrElse(\"<hard-code-one>\")\n\nval fs_s3_awsAccessKeyId = \"AKIAIG6Z5TCM5V4OCKAA\"\nval fs_s3_awsSecretAccessKey = \"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\"",
      "language" : "scala",
      "collapsed" : false,
      "prompt_number" : 25,
      "outputs" : [ ]
    }, {
      "cell_type" : "code",
      "input" : "val adamFileOnS3 = \"s3://med-at-scale/1000genomes/\"",
      "language" : "scala",
      "collapsed" : false,
      "prompt_number" : 21,
      "outputs" : [ ]
    }, {
      "cell_type" : "markdown",
      "source" : "Setting SPark with ADAM libs"
    }, {
      "cell_type" : "code",
      "input" : ":dp org.bdgenomics.adam % adam-apis % 0.15.0\n- org.apache.hadoop % hadoop-client %   _\n- org.apache.spark  %     _         %   _\n- org.scala-lang    %     _         %   _\n- org.scoverage     %     _         %   _",
      "language" : "scala",
      "collapsed" : false,
      "prompt_number" : 6,
      "outputs" : [ ]
    }, {
      "cell_type" : "markdown",
      "source" : "Update Spark configuration to cope with ADAM's needs"
    }, {
      "cell_type" : "code",
      "input" : "reset(lastChanges = _.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n                     .set(\"spark.kryo.registrator\", \"org.bdgenomics.adam.serialization.ADAMKryoRegistrator\")\n                     .set(\"spark.kryoserializer.buffer.mb\", \"4\")\n                     .set(\"spark.kryo.referenceTracking\", \"true\")\n      \t\t\t\t .setMaster(s\"spark://$master:7077\")\n                     .setAppName(\"ADAM 1000genomes\")\n                     .set(\"spark.executor.memory\", \"13g\")\n                     .set(\"fs.s3.awsAccessKeyId\", fs_s3_awsAccessKeyId)\n                     .set(\"fs.s3.awsSecretAccessKey\", fs_s3_awsSecretAccessKey)\n)",
      "language" : "scala",
      "collapsed" : false,
      "prompt_number" : 8,
      "outputs" : [ ]
    }, {
      "cell_type" : "markdown",
      "source" : "Imports all Spark and ADAM package and types we'll after."
    }, {
      "cell_type" : "code",
      "input" : "import org.apache.hadoop.fs.{FileSystem, Path}\n\nimport org.bdgenomics.adam.converters.{ VCFLine, VCFLineConverter, VCFLineParser }\nimport org.bdgenomics.formats.avro.{Genotype, FlatGenotype}\nimport org.bdgenomics.adam.models.VariantContext\nimport org.bdgenomics.adam.rdd.ADAMContext._\nimport org.bdgenomics.adam.rdd.variation.VariationContext._\nimport org.bdgenomics.adam.rdd.ADAMContext\n  \nimport org.apache.spark.rdd.RDD",
      "language" : "scala",
      "collapsed" : false,
      "prompt_number" : 9,
      "outputs" : [ ]
    }, {
      "cell_type" : "code",
      "input" : "val gts:RDD[Genotype] = sparkContext.adamLoad(adamFileOnS3)",
      "language" : "scala",
      "collapsed" : false,
      "prompt_number" : 24,
      "outputs" : [ ]
    }, {
      "cell_type" : "markdown",
      "source" : "Real test → counting how many samples we have in the data"
    }, {
      "cell_type" : "code",
      "input" : "val sampleCount = gts.map(_.getSampleId.toString.hashCode).distinct.count\ns\"#Samples: $sampleCount\"",
      "language" : "scala",
      "collapsed" : false,
      "prompt_number" : 23,
      "outputs" : [ ]
    }, {
      "cell_type" : "code",
      "input" : "",
      "language" : "scala",
      "collapsed" : true,
      "outputs" : [ ]
    } ]
  } ],
  "autosaved" : [ {
    "cells" : [ {
      "cell_type" : "code",
      "input" : ":local-repo /tmp/spark-notebook/repo",
      "language" : "scala",
      "collapsed" : false,
      "prompt_number" : 1,
      "outputs" : [ ]
    }, {
      "cell_type" : "code",
      "input" : "sparkContext.stop",
      "language" : "scala",
      "collapsed" : false,
      "prompt_number" : 2,
      "outputs" : [ ]
    }, {
      "cell_type" : "code",
      "input" : "import sys.process._\nval master = (\"ec2-metadata --public-hostname\"!!).drop(\"public-hostname: \".size).mkString.trim",
      "language" : "scala",
      "collapsed" : false,
      "prompt_number" : 3,
      "outputs" : [ ]
    }, {
      "cell_type" : "code",
      "input" : "val fs_s3_awsAccessKeyId      = sys.env.get(\"AWS_ACCESS_KEY_ID\").getOrElse(\"<hard-code-one>\")\nval fs_s3_awsSecretAccessKey  = sys.env.get(\"AWS_SECRET_ACCESS_KEY\").getOrElse(\"<hard-code-one>\")",
      "language" : "scala",
      "collapsed" : false,
      "prompt_number" : 14,
      "outputs" : [ ]
    }, {
      "cell_type" : "code",
      "input" : "val adamFileOnS3 = \"s3://med-at-scale/1000genomes/ALL.chr1.integrated_phase1_v3.20101123.snps_indels_svs.genotypes.vcf.adam\"",
      "language" : "scala",
      "collapsed" : false,
      "prompt_number" : 7,
      "outputs" : [ ]
    }, {
      "cell_type" : "markdown",
      "source" : "Setting SPark with ADAM libs"
    }, {
      "cell_type" : "code",
      "input" : ":dp org.bdgenomics.adam % adam-apis % 0.15.0\n- org.apache.hadoop % hadoop-client %   _\n- org.apache.spark  %     _         %   _\n- org.scala-lang    %     _         %   _\n- org.scoverage     %     _         %   _",
      "language" : "scala",
      "collapsed" : false,
      "prompt_number" : 6,
      "outputs" : [ ]
    }, {
      "cell_type" : "markdown",
      "source" : "Update Spark configuration to cope with ADAM's needs"
    }, {
      "cell_type" : "code",
      "input" : "reset(lastChanges = _.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n                     .set(\"spark.kryo.registrator\", \"org.bdgenomics.adam.serialization.ADAMKryoRegistrator\")\n                     .set(\"spark.kryoserializer.buffer.mb\", \"4\")\n                     .set(\"spark.kryo.referenceTracking\", \"true\")\n      \t\t\t\t .setMaster(s\"spark://$master:7077\")\n                     .setAppName(\"ADAM 1000genomes\")\n                     .set(\"spark.executor.memory\", \"13g\")\n                     .set(\"fs.s3.awsAccessKeyId\", fs_s3_awsAccessKeyId)\n                     .set(\"fs.s3.awsSecretAccessKey\", fs_s3_awsSecretAccessKey)\n)",
      "language" : "scala",
      "collapsed" : false,
      "prompt_number" : 8,
      "outputs" : [ ]
    }, {
      "cell_type" : "markdown",
      "source" : "Imports all Spark and ADAM package and types we'll after."
    }, {
      "cell_type" : "code",
      "input" : "import org.apache.hadoop.fs.{FileSystem, Path}\n\nimport org.bdgenomics.adam.converters.{ VCFLine, VCFLineConverter, VCFLineParser }\nimport org.bdgenomics.formats.avro.{Genotype, FlatGenotype}\nimport org.bdgenomics.adam.models.VariantContext\nimport org.bdgenomics.adam.rdd.ADAMContext._\nimport org.bdgenomics.adam.rdd.variation.VariationContext._\nimport org.bdgenomics.adam.rdd.ADAMContext\n  \nimport org.apache.spark.rdd.RDD",
      "language" : "scala",
      "collapsed" : false,
      "prompt_number" : 9,
      "outputs" : [ ]
    }, {
      "cell_type" : "code",
      "input" : "val gts:RDD[Genotype] = sparkContext.adamLoad(adamFileOnS3)",
      "language" : "scala",
      "collapsed" : false,
      "prompt_number" : 11,
      "outputs" : [ ]
    }, {
      "cell_type" : "markdown",
      "source" : "Real test → counting how many samples we have in the data"
    }, {
      "cell_type" : "code",
      "input" : "sparkContext.adamVCFLoad(\"s3n://med-at-scale/10_3000.vcf\")",
      "language" : "scala",
      "collapsed" : false,
      "prompt_number" : 17,
      "outputs" : [ ]
    }, {
      "cell_type" : "code",
      "input" : "res8.count()",
      "language" : "scala",
      "collapsed" : false,
      "prompt_number" : 18,
      "outputs" : [ ]
    }, {
      "cell_type" : "code",
      "input" : "val sampleCount = gts.map(_.getSampleId.toString.hashCode).distinct.count\ns\"#Samples: $sampleCount\"",
      "language" : "scala",
      "collapsed" : false,
      "prompt_number" : 12,
      "outputs" : [ ]
    }, {
      "cell_type" : "code",
      "input" : "",
      "language" : "scala",
      "collapsed" : true,
      "outputs" : [ ]
    } ]
  } ],
  "nbformat" : 3
}