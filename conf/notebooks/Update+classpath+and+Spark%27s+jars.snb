{
  "metadata" : {
    "name" : "Update classpath and Spark's jars",
    "user_save_timestamp" : "2202-01-31T14:58:20.607Z",
    "auto_save_timestamp" : "2015-01-14T14:53:53.626Z"
  },
  "worksheets" : [ {
    "cells" : [ {
      "cell_type" : "markdown",
      "source" : "### Set the local repo (where jars will be added) "
    }, {
      "cell_type" : "code",
      "input" : ":local-repo /tmp/snb/repository",
      "language" : "scala",
      "collapsed" : false,
      "prompt_number" : 1,
      "outputs" : [ ]
    }, {
      "cell_type" : "markdown",
      "source" : "### Add a remote repo using an `id`, the `type` and its `url`. \n\n_Note:_ Generally, `type` is `default`."
    }, {
      "cell_type" : "code",
      "input" : ":remote-repo oss-sonatype % default % https://oss.sonatype.org/content/repositories/releases/",
      "language" : "scala",
      "collapsed" : false,
      "prompt_number" : 2,
      "outputs" : [ ]
    }, {
      "cell_type" : "markdown",
      "source" : "**Tip:** you can add credentials by appeding the `username` and `password` right after the url.\n\nVery helpful when using S3 repositories where `username` is the key and `password` is the secret.\n\n**Remark:** the `username` and `password` can be provided via env variable (the SparkNotebook's environment)."
    }, {
      "cell_type" : "code",
      "input" : ":remote-repo s3-repo % default % s3://<bucket-name>/<path-to-repo> % (\"$AWS_ACCESS_KEY_ID\", \"$AWS_SECRET_ACCESS_KEY\")",
      "language" : "scala",
      "collapsed" : false,
      "prompt_number" : 2,
      "outputs" : [ ]
    }, {
      "cell_type" : "code",
      "input" : ":dp com.virdata % core-compute-jobs-actualvalues % 1.4.0-SNAPSHOT",
      "language" : "scala",
      "collapsed" : true,
      "prompt_number" : -1,
      "outputs" : [ ]
    }, {
      "cell_type" : "markdown",
      "source" : "### Declare some dependencies require for the analysis."
    }, {
      "cell_type" : "markdown",
      "source" : "* line starting with `+` or nothing are considered as __includes__\n* lines starting with a `-` are considered to be __excluded__ (from all other includes transitives deps)"
    }, {
      "cell_type" : "markdown",
      "source" : "__`dp` stands for dependencies, hence the spark context will contain all of them (sent to workers)__ "
    }, {
      "cell_type" : "code",
      "input" : ":dp org.bdgenomics.adam % adam-apis % 0.15.0\n- org.apache.hadoop % hadoop-client %   _\n- org.apache.spark  %     _         %   _\n- org.scala-lang    %     _         %   _\n- org.scoverage     %     _         %   _",
      "language" : "scala",
      "collapsed" : false,
      "prompt_number" : 12,
      "outputs" : [ ]
    }, {
      "cell_type" : "markdown",
      "source" : "### Check the spark context â†’ the configuration's key `spark.jars` contains all dependencies!"
    }, {
      "cell_type" : "code",
      "input" : "sparkContext.getConf.toDebugString",
      "language" : "scala",
      "collapsed" : false,
      "prompt_number" : 13,
      "outputs" : [ ]
    } ]
  } ],
  "nbformat" : 3
}