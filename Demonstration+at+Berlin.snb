{
  "metadata":{
    "name":"Demonstration at Berlin",
    "user_save_timestamp":"2014-11-14T16:45:10.593Z",
    "auto_save_timestamp":"2014-11-15T09:32:50.765Z"
  },
  "worksheets":[{
    "cells":[{
      "cell_type":"code",
      "input":"// We have a spark context like when running spark-shell in spark/bin\nsparkContext",
      "language":"scala",
      "collapsed":false,
      "prompt_number":39,
      "outputs":[]
    },{
      "cell_type":"code",
      "input":"// Create an artificial RDD (abstraction for distributed data sets) of numbers 1 to 1000\nval rdd = sparkContext.parallelize(1 to 1000)\nrdd.count()  ",
      "language":"scala",
      "collapsed":false,
      "prompt_number":40,
      "outputs":[]
    },{
      "cell_type":"code",
      "input":"// Just take numbers into account that are divisible by 3 then sum up all remaining numbers.\nrdd.filter(_ % 3 == 0)\n   .reduce(_ + _)",
      "language":"scala",
      "collapsed":false,
      "prompt_number":41,
      "outputs":[]
    },{
      "cell_type":"code",
      "input":"// We could also load data from hdfs (hadoop file system), parquet files, s3, text files (csv)...\n// but what if we want to visualize subsets of the data?\n// it is often the case that data analysis projects start with taking a look at the data in an interactive way",
      "language":"scala",
      "collapsed":false,
      "outputs":[]
    }]
  }],
  "autosaved":[{
    "cells":[{
      "cell_type":"code",
      "input":"// We have a spark context like when running spark-shell in spark/bin\nsparkContext",
      "language":"scala",
      "collapsed":false,
      "prompt_number":1,
      "outputs":[]
    },{
      "cell_type":"code",
      "input":"// Create an artificial RDD (abstraction for distributed data sets) of numbers 1 to 1000\nval rdd = sparkContext.parallelize(1 to 1000)\nrdd.count()  ",
      "language":"scala",
      "collapsed":false,
      "prompt_number":2,
      "outputs":[]
    },{
      "cell_type":"code",
      "input":"// Just take numbers into account that are divisible by 3 then sum up all remaining numbers.\nrdd.filter(_ % 3 == 0)\n   .reduce(_ + _)",
      "language":"scala",
      "collapsed":false,
      "prompt_number":3,
      "outputs":[]
    },{
      "cell_type":"code",
      "input":"// We could also load data from hdfs (hadoop file system), parquet files, s3, text files (csv)...\n// but what if we want to visualize subsets of the data?\n// it is often the case that data analysis projects start with taking a look at the data in an interactive way",
      "language":"scala",
      "collapsed":false,
      "prompt_number":5,
      "outputs":[]
    }]
  }],
  "nbformat":3
}